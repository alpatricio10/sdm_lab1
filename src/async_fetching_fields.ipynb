{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48662de0-b38a-4194-bba9-6ce84cdfeeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import csv\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "from semanticscholar import AsyncSemanticScholar \n",
    "from semanticscholar.Paper import Paper as S2Paper\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# LOAD PAPER IDs + References from CSV\n",
    "# -------------------------------------------\n",
    "def load_papers_and_references_from_csv(filename, paper_id_col=\"paperId\", references_col=\"references\"):\n",
    "    paper_ids = []\n",
    "    references_map = {}\n",
    "\n",
    "    with open(filename, newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            pid = row.get(paper_id_col, \"\").strip()\n",
    "            if not pid:\n",
    "                continue\n",
    "\n",
    "            refs_str = row.get(references_col, \"\").strip()\n",
    "            refs_list = []\n",
    "            if refs_str:\n",
    "                refs_list = [r.strip() for r in refs_str.split(\";\") if r.strip()]\n",
    "\n",
    "            paper_ids.append(pid)\n",
    "            references_map[pid] = refs_list\n",
    "\n",
    "    return paper_ids, references_map\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# ASYNC BATCH FETCH, but exclude references\n",
    "# -------------------------------------------\n",
    "async def fetch_papers_batch(sch, paper_ids_batch, fields, return_not_found=False):\n",
    "    return await sch.get_papers(\n",
    "        paper_ids=paper_ids_batch,\n",
    "        fields=fields,\n",
    "        return_not_found=return_not_found\n",
    "    )\n",
    "\n",
    "async def fetch_all_papers_async(paper_ids, fields=None, concurrency=5, chunk_size=500, return_not_found=False):\n",
    "    \"\"\"\n",
    "    We will NOT ask for \"references\" in the fields,\n",
    "    because we want to keep the references from the CSV only.\n",
    "    \"\"\"\n",
    "    if fields is None:\n",
    "        fields = [\n",
    "            \"title\",\n",
    "            \"abstract\",\n",
    "            \"year\",\n",
    "            \"venue\",\n",
    "            \"authors\",\n",
    "            \"citationCount\",\n",
    "            \"publicationTypes\",\n",
    "            \"externalIds\",\n",
    "            \"fieldsOfStudy\",\n",
    "            \"journal\",\n",
    "            \"url\",\n",
    "        ]\n",
    "\n",
    "    sch = AsyncSemanticScholar()\n",
    "\n",
    "    all_papers = []\n",
    "    not_found_ids = []\n",
    "\n",
    "    sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "    async def worker(batch):\n",
    "        async with sem:\n",
    "            return await fetch_papers_batch(sch, batch, fields, return_not_found=return_not_found)\n",
    "\n",
    "    tasks = []\n",
    "    for i in range(0, len(paper_ids), chunk_size):\n",
    "        chunk = paper_ids[i : i + chunk_size]\n",
    "        task = asyncio.create_task(worker(chunk))\n",
    "        tasks.append(task)\n",
    "\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    if return_not_found:\n",
    "        for res in results:\n",
    "            if isinstance(res, Exception):\n",
    "                print(\"[Paper Batch Error]\", res)\n",
    "                continue\n",
    "            papers_list, nf_list = res\n",
    "            all_papers.extend(papers_list)\n",
    "            not_found_ids.extend(nf_list)\n",
    "        return all_papers, not_found_ids\n",
    "    else:\n",
    "        for res in results:\n",
    "            if isinstance(res, Exception):\n",
    "                print(\"[Paper Batch Error]\", res)\n",
    "                continue\n",
    "            all_papers.extend(res)\n",
    "        return all_papers\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# AUTHOR BATCH (unchanged)\n",
    "# -------------------------------------------\n",
    "async def fetch_authors_batch(sch, author_ids_batch, fields, return_not_found=False):\n",
    "    return await sch.get_authors(\n",
    "        author_ids=author_ids_batch,\n",
    "        fields=fields,\n",
    "        return_not_found=return_not_found\n",
    "    )\n",
    "\n",
    "async def fetch_all_authors_async(author_ids, fields=None, concurrency=10, chunk_size=100, return_not_found=False):\n",
    "    if fields is None:\n",
    "        fields = [\n",
    "            \"name\",\n",
    "            \"affiliations\",\n",
    "            \"homepage\",\n",
    "            \"paperCount\",\n",
    "            \"citationCount\",\n",
    "            \"hIndex\",\n",
    "            \"papers\"\n",
    "        ]\n",
    "\n",
    "    sch = AsyncSemanticScholar()\n",
    "    all_authors = []\n",
    "    not_found_ids = []\n",
    "\n",
    "    sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "    async def worker(batch):\n",
    "        async with sem:\n",
    "            return await fetch_authors_batch(sch, batch, fields, return_not_found=return_not_found)\n",
    "\n",
    "    tasks = []\n",
    "    for i in range(0, len(author_ids), chunk_size):\n",
    "        chunk = author_ids[i : i + chunk_size]\n",
    "        task = asyncio.create_task(worker(chunk))\n",
    "        tasks.append(task)\n",
    "\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    if return_not_found:\n",
    "        for res in results:\n",
    "            if isinstance(res, Exception):\n",
    "                print(\"[Author Batch Error]\", res)\n",
    "                continue\n",
    "            authors_list, nf_list = res\n",
    "            all_authors.extend(authors_list)\n",
    "            not_found_ids.extend(nf_list)\n",
    "        return all_authors, not_found_ids\n",
    "    else:\n",
    "        for res in results:\n",
    "            if isinstance(res, Exception):\n",
    "                print(\"[Author Batch Error]\", res)\n",
    "                continue\n",
    "            all_authors.extend(res)\n",
    "        return all_authors\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# CSV WRITERS (same as before)\n",
    "# -------------------------------------------\n",
    "def save_papers_to_csv(papers, filename=\"papers.csv\"):\n",
    "    fieldnames = [\n",
    "        \"paperId\",\n",
    "        \"title\",\n",
    "        \"abstract\",\n",
    "        \"doi\",\n",
    "        \"url\",\n",
    "        \"citationCount\",\n",
    "        \"venue\",\n",
    "        \"venueType\",\n",
    "        \"year\",\n",
    "        \"fieldsOfStudy\",\n",
    "        \"pages\",\n",
    "        \"references\",\n",
    "        \"authors\"\n",
    "    ]\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for p in papers:\n",
    "            pid = p.paperId\n",
    "            title = p.title\n",
    "            abstract = p.abstract\n",
    "            url = p.url\n",
    "            year = p.year\n",
    "            venue = p.venue\n",
    "            citation_count = getattr(p, \"citationCount\", 0)\n",
    "\n",
    "            # doi from externalIds\n",
    "            doi = \"\"\n",
    "            if getattr(p, \"externalIds\", None):\n",
    "                doi = p.externalIds.get(\"DOI\") or p.externalIds.get(\"doi\") or \"\"\n",
    "\n",
    "            # Fields of study\n",
    "            fos = \"\"\n",
    "            if getattr(p, \"fieldsOfStudy\", None):\n",
    "                fos = \"; \".join(p.fieldsOfStudy)\n",
    "\n",
    "            # references\n",
    "            ref_str = \"\"\n",
    "            if getattr(p, \"references\", None):\n",
    "                ref_str = \"; \".join(r.paperId for r in p.references if r.paperId)\n",
    "\n",
    "            # authors\n",
    "            authors_str = \"\"\n",
    "            if getattr(p, \"authors\", None):\n",
    "                authors_str = \"; \".join(a.authorId or \"\" for a in p.authors if a.authorId)\n",
    "\n",
    "            # pages from p.journal if available\n",
    "            pages = \"\"\n",
    "            if getattr(p, \"journal\", None) and p.journal:\n",
    "                pages = getattr(p.journal, \"pages\", \"\") or \"\"\n",
    "\n",
    "            # venueType\n",
    "            venue_type = \"Unknown\"\n",
    "            if getattr(p, \"publicationTypes\", None):\n",
    "                if \"JournalArticle\" in p.publicationTypes:\n",
    "                    venue_type = \"Journal\"\n",
    "                elif \"Conference\" in p.publicationTypes:\n",
    "                    venue_type = \"Conference\"\n",
    "\n",
    "            row = {\n",
    "                \"paperId\": pid,\n",
    "                \"title\": title,\n",
    "                \"abstract\": abstract,\n",
    "                \"doi\": doi,\n",
    "                \"url\": url,\n",
    "                \"citationCount\": citation_count,\n",
    "                \"venue\": venue,\n",
    "                \"venueType\": venue_type,\n",
    "                \"year\": year,\n",
    "                \"fieldsOfStudy\": fos,\n",
    "                \"pages\": pages,\n",
    "                \"references\": ref_str,\n",
    "                \"authors\": authors_str\n",
    "            }\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"[save_papers_to_csv] Wrote {len(papers)} papers to {os.path.abspath(filename)}\")\n",
    "\n",
    "def save_authors_to_csv(authors, filename=\"authors.csv\"):\n",
    "    \"\"\"\n",
    "    Writes out only the columns: authorId, name, affiliations\n",
    "    \"\"\"\n",
    "    fieldnames = [\n",
    "        \"authorId\",\n",
    "        \"name\",\n",
    "        \"affiliations\",\n",
    "    ]\n",
    "\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for a in authors:\n",
    "            row = {\n",
    "                \"authorId\": a.authorId,\n",
    "                \"name\": a.name or \"\",\n",
    "                \"affiliations\": \"\",\n",
    "            }\n",
    "            if getattr(a, \"affiliations\", None):\n",
    "                row[\"affiliations\"] = \"; \".join(a.affiliations)\n",
    "\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"[save_authors_to_csv] Wrote {len(authors)} authors to {os.path.abspath(filename)}\")\n",
    "\n",
    "def save_venues_to_csv(venues, filename=\"venues.csv\"):\n",
    "    fieldnames = [\"venueType\", \"name\", \"volume\", \"pages\"]\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for v in venues:\n",
    "            writer.writerow(v)\n",
    "    print(f\"[save_venues_to_csv] Wrote {len(venues)} venues to {os.path.abspath(filename)}\")\n",
    "\n",
    "def save_paper_keywords_csv(papers, filename=\"paper_keywords.csv\"):\n",
    "    \"\"\"\n",
    "    Writes out each paper's fieldsOfStudy (a.k.a. 'keywords') \n",
    "    in a separate CSV with columns: paperId, keyword.\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    fieldnames = [\"paperId\", \"keyword\"]\n",
    "    count = 0\n",
    "\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for p in papers:\n",
    "            pid = p.paperId\n",
    "            if getattr(p, \"fieldsOfStudy\", None):\n",
    "                for kw in p.fieldsOfStudy:\n",
    "                    row = {\n",
    "                        \"paperId\": pid,\n",
    "                        \"keyword\": kw\n",
    "                    }\n",
    "                    writer.writerow(row)\n",
    "                    count += 1\n",
    "\n",
    "    print(f\"[save_paper_keywords_csv] Wrote {count} (paperId, keyword) pairs to {filename}\")\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# MAIN\n",
    "# -------------------------------------------\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    1) Load paper IDs + references from step1_papers.csv\n",
    "    2) Fetch everything but references from the S2 API\n",
    "    3) Overwrite the paper's references with those from step1\n",
    "    4) Extract author info & venue info\n",
    "    5) Fetch authors in parallel\n",
    "    6) Write out final CSVs\n",
    "    \"\"\"\n",
    "\n",
    "    input_csv = \"papers_combined.csv\"\n",
    "    paper_ids, references_map = load_papers_and_references_from_csv(\n",
    "        filename=input_csv, \n",
    "        paper_id_col=\"paperId\", \n",
    "        references_col=\"references\"\n",
    "    )\n",
    "    print(f\"[main] Loaded {len(paper_ids)} paper IDs from {input_csv}.\")\n",
    "\n",
    "    all_papers, not_found = await fetch_all_papers_async(\n",
    "        paper_ids=paper_ids,\n",
    "        fields=None,  \n",
    "        concurrency=10,\n",
    "        chunk_size=500,\n",
    "        return_not_found=True\n",
    "    )\n",
    "    print(f\"[main] Fetched {len(all_papers)} papers. Not found: {len(not_found)}\")\n",
    "\n",
    "\n",
    "    from semanticscholar import Paper\n",
    "\n",
    "    paper_dict = {}\n",
    "    for p in all_papers:\n",
    "        paper_dict[p.paperId] = p\n",
    "\n",
    "\n",
    "    for pid, old_paper in paper_dict.items():\n",
    "        old_refs = references_map.get(pid, [])\n",
    "        \n",
    "        data_copy = copy.deepcopy(old_paper.raw_data)\n",
    "        \n",
    "        data_copy[\"references\"] = []\n",
    "        for rid in old_refs:\n",
    "            data_copy[\"references\"].append({\"paperId\": rid})\n",
    "    \n",
    "        new_paper = S2Paper(data_copy)\n",
    "        paper_dict[pid] = new_paper\n",
    "\n",
    "    final_papers = list(paper_dict.values())\n",
    "\n",
    "    author_ids_set = set()\n",
    "    venues_set = set()\n",
    "\n",
    "    for p in final_papers:\n",
    "        if getattr(p, \"authors\", None):\n",
    "            for a in p.authors:\n",
    "                if a.authorId:\n",
    "                    author_ids_set.add(a.authorId)\n",
    "        venue_type = \"Unknown\"\n",
    "        if getattr(p, \"publicationTypes\", None):\n",
    "            pub_types = p.publicationTypes or []\n",
    "            if \"JournalArticle\" in pub_types:\n",
    "                venue_type = \"Journal\"\n",
    "            elif \"Conference\" in pub_types:\n",
    "                venue_type = \"Conference\"\n",
    "        \n",
    "        name = p.venue or \"\"\n",
    "        volume = \"\"\n",
    "        pages = \"\"\n",
    "        \n",
    "        if venue_type == \"Journal\":\n",
    "            conf_words = [\"conference\", \"workshop\", \"proceedings\", \"symposium\"]\n",
    "            name_lower = name.lower()\n",
    "            if any(cw in name_lower for cw in conf_words):\n",
    "                venue_type = \"Conference\"\n",
    "        \n",
    "        if venue_type == \"Journal\":\n",
    "            if getattr(p, \"journal\", None) and p.journal:\n",
    "                if getattr(p.journal, \"name\", None):\n",
    "                    name = p.journal.name\n",
    "                volume = getattr(p.journal, \"volume\", \"\") or \"\"\n",
    "                pages = getattr(p.journal, \"pages\", \"\") or \"\"\n",
    "        \n",
    "        if name:\n",
    "            vdict = {\n",
    "                \"venueType\": venue_type,\n",
    "                \"name\": name,\n",
    "                \"volume\": volume,\n",
    "                \"pages\": pages\n",
    "            }\n",
    "            venues_set.add(tuple(vdict.items()))\n",
    "\n",
    "\n",
    "    unique_venues = [dict(t) for t in venues_set]\n",
    "\n",
    "    print(f\"[main] Found {len(author_ids_set)} unique authors, {len(unique_venues)} unique venues.\")\n",
    "\n",
    "    author_ids = list(author_ids_set)\n",
    "    all_authors, not_found_authors = await fetch_all_authors_async(\n",
    "        author_ids,\n",
    "        fields=None,\n",
    "        concurrency=10,\n",
    "        chunk_size=100,\n",
    "        return_not_found=True\n",
    "    )\n",
    "    print(f\"[main] Fetched {len(all_authors)} authors. Not found = {len(not_found_authors)}\")\n",
    "\n",
    "    save_papers_to_csv(final_papers, \"papers.csv\")\n",
    "    save_authors_to_csv(all_authors, \"authors.csv\")\n",
    "    save_venues_to_csv(unique_venues, \"venues.csv\")\n",
    "    save_paper_keywords_csv(final_papers, \"paper_keywords.csv\")\n",
    "\n",
    "\n",
    "    print(\"[main] Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0e3a27-6ad8-4338-84ba-474c2b8eff43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] Loaded 18544 paper IDs from papers_combined.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:semanticscholar:IDs not found: ['5db816340e0a63e8ebaa87fad9aed9123f1163f5']\n",
      "WARNING:semanticscholar:IDs not found: ['94a81307056161cc26ebd3e11078902e04bc3f4a']\n",
      "WARNING:semanticscholar:IDs not found: ['0d9deb3808919cb33ae9f9ae3ac1fdf4ebb9269f']\n",
      "WARNING:semanticscholar:IDs not found: ['e94fa9bebdfd08c13785385be3975e23dc2756ca']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] Fetched 18544 papers. Not found: 4\n",
      "[main] Found 61522 unique authors, 13681 unique venues.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:semanticscholar:IDs not found: ['2246039260']\n",
      "WARNING:semanticscholar:IDs not found: ['2148323429']\n",
      "WARNING:semanticscholar:IDs not found: ['2148613752']\n",
      "WARNING:semanticscholar:IDs not found: ['2148480601']\n",
      "WARNING:semanticscholar:IDs not found: ['2148410695']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Network error communicating with endpoint\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Network error communicating with endpoint\n",
      "[Author Batch Error] Network error communicating with endpoint\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Network error communicating with endpoint\n",
      "[Author Batch Error] Network error communicating with endpoint\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Network error communicating with endpoint\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] \n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Network error communicating with endpoint\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[Author Batch Error] Endpoint request timed out\n",
      "[main] Fetched 55217 authors. Not found = 5\n",
      "[save_papers_to_csv] Wrote 18408 papers to /Users/olhabaliasina/Documents/BDMA/2nd_semester/Semantic Data Management/test_import/papers.csv\n",
      "[save_authors_to_csv] Wrote 55217 authors to /Users/olhabaliasina/Documents/BDMA/2nd_semester/Semantic Data Management/test_import/authors.csv\n",
      "[save_venues_to_csv] Wrote 13681 venues to /Users/olhabaliasina/Documents/BDMA/2nd_semester/Semantic Data Management/test_import/venues.csv\n",
      "[save_paper_keywords_csv] Wrote 23109 (paperId, keyword) pairs to paper_keywords.csv\n",
      "[main] Done.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a222d4-c128-45ec-8a03-68ae63a63aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7482923-d524-45a1-a57d-1a51e2d81c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1266ef91-9b77-4b12-9aed-be09f0874adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650ee40c-f200-4f61-9cb8-db856b8a9841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5273b2f-3c42-48c8-8cc2-dc5b37576947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58aadc8-98ea-464e-af11-145da1faa2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (omnixai)",
   "language": "python",
   "name": "omnixai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
